{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import statistics\n",
    "import tqdm\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve,f1_score, fbeta_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "#from yellowbrick.model_selection import FeatureImportances\n",
    "import matplotlib.gridspec as gridspec\n",
    "#from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import defaultdict\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**After Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open('/home/nick/Documents/data/select_cols.p', 'rb') as read_file:\n",
    "    selected_columns = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "selected_columns\n",
    "tw = selected_columns.sample(50000)\n",
    "ttw = selected_columns.sample(1000)\n",
    "tttw = selected_columns.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UHRSWORK</th>\n",
       "      <th>OCC</th>\n",
       "      <th>DEGFIELDD</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>WKSWORK2</th>\n",
       "      <th>VALUEH</th>\n",
       "      <th>OWNCOST</th>\n",
       "      <th>DEGFIELD</th>\n",
       "      <th>EDUCD</th>\n",
       "      <th>AGE</th>\n",
       "      <th>...</th>\n",
       "      <th>WRKLSTWK</th>\n",
       "      <th>PWMET13</th>\n",
       "      <th>REGION</th>\n",
       "      <th>EMPSTATD</th>\n",
       "      <th>WORKEDYR</th>\n",
       "      <th>CBPERNUM</th>\n",
       "      <th>PWMET13ERR</th>\n",
       "      <th>YNGCH</th>\n",
       "      <th>BEDROOMS</th>\n",
       "      <th>&gt;50K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1837946</th>\n",
       "      <td>40.0</td>\n",
       "      <td>4750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>2030.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3153267</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85000.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673538</th>\n",
       "      <td>30.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33100.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356044</th>\n",
       "      <td>45.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40140.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071122</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>1154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UHRSWORK     OCC  DEGFIELDD  EDUC  WKSWORK2    VALUEH  OWNCOST  \\\n",
       "1837946      40.0  4750.0        0.0   6.0       3.0  250000.0   2030.0   \n",
       "3153267       0.0     0.0        0.0   6.0       0.0   85000.0    603.0   \n",
       "673538       30.0  7200.0        0.0   2.0       6.0  150000.0    584.0   \n",
       "356044       45.0   110.0        0.0   7.0       6.0  250000.0    425.0   \n",
       "2071122       0.0     0.0        0.0   6.0       0.0  500000.0   1154.0   \n",
       "\n",
       "         DEGFIELD  EDUCD   AGE  ...  WRKLSTWK  PWMET13  REGION  EMPSTATD  \\\n",
       "1837946       0.0   63.0  53.0  ...       2.0      0.0    12.0      10.0   \n",
       "3153267       0.0   64.0  65.0  ...       1.0      0.0    21.0      30.0   \n",
       "673538        0.0   23.0  56.0  ...       2.0  33100.0    31.0      10.0   \n",
       "356044        0.0   71.0  64.0  ...       2.0  40140.0    42.0      10.0   \n",
       "2071122       0.0   64.0  80.0  ...       1.0      0.0    12.0      30.0   \n",
       "\n",
       "         WORKEDYR  CBPERNUM  PWMET13ERR  YNGCH  BEDROOMS  >50K  \n",
       "1837946       3.0       1.0         0.0   13.0       4.0   0.0  \n",
       "3153267       1.0       2.0         0.0   99.0       5.0   0.0  \n",
       "673538        3.0       1.0         3.0   23.0       5.0   0.0  \n",
       "356044        3.0       1.0         1.0   99.0       4.0   1.0  \n",
       "2071122       1.0       2.0         0.0   99.0       5.0   0.0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X, Xs, y, ys = selected_columns.drop(columns=['>50K', 'YRMARR', 'PWTYPE', 'BPLD', 'DEGFIELDD', 'EDUCD', 'PWMET13', 'BIRTHYR', 'SLWT', 'BPLD', 'RELATED', 'ANCESTR1D', 'EMPSTATD', 'PWMET13ERR', 'COUNTYICP']), tw.drop(columns=['>50K', 'YRMARR', 'PWTYPE', 'BPLD', 'DEGFIELDD', 'EDUCD', 'PWMET13', 'BIRTHYR', 'SLWT', 'BPLD', 'RELATED', 'ANCESTR1D', 'EMPSTATD', 'PWMET13ERR', 'COUNTYICP']), selected_columns.filter(['>50K']), tw.filter(['>50K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = selected_columns.filter(['EDUC', 'UHRSWORK', 'OCC', 'VALUEH', 'DEGFIELD', 'AGE', 'SEX', 'RACE', 'TRANWORK'])\n",
    "Zs = tw.filter(['EDUC', 'UHRSWORK', 'OCC', 'VALUEH', 'DEGFIELD', 'AGE', 'SEX', 'RACE', 'TRANWORK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['EDUC', 'DEGFIELD', 'SEX', 'OCC']\n",
    "continuous_features = ['UHRSWORK', 'VALUEH', 'AGE', 'TRANWORK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, ys, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = X_train.copy()\n",
    "train_df['$$$'] = y_train\n",
    "\n",
    "Z_train, Z_test, q_train, q_test = train_test_split(Z, y, test_size=0.2, random_state=42)\n",
    "Zs_train, Zs_test, qs_train, qs_test = train_test_split(Zs, ys, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = StandardScaler()\n",
    "X_train_scaled = std.fit_transform(X_train)\n",
    "Xs_train_scaled = std.fit_transform(Xs_train)\n",
    "\n",
    "Z_train_scaled = std.fit_transform(Z_train)\n",
    "Zs_train_scaled = std.fit_transform(Zs_train)\n",
    "\n",
    "len(Xs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN With regular X nonscaled\n",
    "Full dataset, takes too long for quick testing. Save for final product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn = KNeighborsClassifier(n_neighbors=5)\n",
    "# knn.fit(X_train, y_train)\n",
    "# print(\"The score for kNN is\")\n",
    "# print(\"Training: {:6.2f}%\".format(100*knn.score(X_train, y_train)))\n",
    "# print(\"Test set: {:6.2f}%\".format(100*knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-07a8951d040d>:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  knn.fit(Xs_train, ys_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for kNN is\n",
      "Training:  75.62%\n",
      "Test set:  64.37%\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(Xs_train, ys_train)\n",
    "print(\"The score for kNN is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*knn.score(Xs_train, ys_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*knn.score(Xs_test, ys_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with scaled Xs \n",
    "\n",
    "\\~ takes forever for some reason \\~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5d64283cb0b9>:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  knn.fit(Xs_train_scaled, ys_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for kNN is\n",
      "Training:  79.00%\n",
      "Test set:  64.00%\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(Xs_train_scaled, ys_train)\n",
    "print(\"The score for kNN is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*knn.score(Xs_train_scaled[:100], ys_train[:100])))\n",
    "print(\"Test set: {:6.2f}%\".format(100*knn.score(Xs_test[:100], ys_test[:100])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with small feature set sample (Zs) scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d65cfc3f2cfb>:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  knn.fit(Zs_train_scaled, qs_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  85.15%\n",
      "Test set:  69.42%\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(Zs_train_scaled, qs_train)\n",
    "print(\"Training: {:6.2f}%\".format(100*knn.score(Zs_train_scaled, qs_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*knn.score(Zs_test, qs_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with small feature set Zs unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\~\\~\\~ *one of the best tried yet* \\~\\~\\~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-be8dd8461117>:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  knn.fit(Zs_train, qs_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for kNN is\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(Zs_train, qs_train)\n",
    "print(\"The score for kNN is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*knn.score(Zs_train, qs_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*knn.score(Zs_test, qs_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Xs non-scaled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C = 0.95)\n",
    "logit.fit(Xs_train, ys_train)\n",
    "print(\"The score for logistic regression is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*logit.score(Xs_train, ys_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*logit.score(Xs_test, ys_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Xs scaled with varying regularization strengths \n",
    "(The varying regularization strengths actually seem to have no effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Cs = [0.5, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "\n",
    "for c in Cs:\n",
    "    logit = LogisticRegression(C = c)\n",
    "    logit.fit(Xs_train_scaled, ys_train)\n",
    "    print(f\"The score for logistic regression with C = {c} is\")\n",
    "    print(\"Training: {:6.2f}%\".format(100*logit.score(Xs_train_scaled, ys_train)))\n",
    "    print(\"Test set: {:6.2f}%\".format(100*logit.score(Xs_test, ys_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "knn_confusion = confusion_matrix(ys_test, knn.predict(Zs_test))\n",
    "plt.figure(dpi=90)\n",
    "sns.heatmap(knn_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='d')\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('kNN confusion matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "\n",
    "\n",
    "cnn = CondensedNearestNeighbour()\n",
    "\n",
    "# X_resampled, y_resampled = cnn.fit_resample(X, y)\n",
    "# Xs_resampled, ys_resampled = cnn.fit_resample(Xs, ys)\n",
    "# Z_resampled, y_resampled = cnn.fit_resample(Z, y)\n",
    "Zs_resampled, ys_resampled = cnn.fit_resample(Zs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sns.countplot(train_df['$$$'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y['>50K'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Slightly unbalanced -- try undersampling. We don't want random undersampling - Condensed Nearest Neighbor looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y_resampled['>50K'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C = 0.95)\n",
    "logit.fit(Xs_train, ys_train)\n",
    "print(\"The score for logistic regression is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*logit.score(X_train, y_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*logit.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C = 0.95)\n",
    "logit.fit(Xs_resampled, ys_resampled)\n",
    "print(\"The score for logistic regression is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*logit.score(Xs_resampled, ys_resampled)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*logit.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# using the default threshold of 0.5, which is what vanilla predict does\n",
    "ys_predict = knn.predict(Zs_test)\n",
    "print(\"Default threshold:\")\n",
    "print(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(ys_test, ys_predict), \n",
    "                                                     recall_score(ys_test, ys_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "knn_confusion = confusion_matrix(ys_test, knn.predict(Zs_test))\n",
    "fig, ax = plt.subplots(dpi=90)\n",
    "#plt.figure()\n",
    "sns.heatmap(knn_confusion, cmap=plt.cm.Blues, annot=True, square=True)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('kNN confusion matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "knn_confusion = confusion_matrix(logit.predict(Xs_test), ys_test)\n",
    "fig, ax = plt.subplots(dpi=90)\n",
    "sns.heatmap(knn_confusion, cmap=plt.cm.Blues, annot=True, square=True)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('kNN confusion matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "fbeta_score(ys_test, ys_predict, average=None, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<a class=\"anchor\" id=\"compare\"></a>\n",
    "**KTAS expert vs KTAS RN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Looks like UHRSWORK and EDUC are the main indicators. Let's see for UHRSWORK how accurate of a prediction we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(Zs_train, ys_train)\n",
    "print(\"The score for kNN is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*knn.score(Zs_train, ys_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*knn.score(Zs_test, ys_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple logistic regression looking the best! Below. Highest r2 on test data yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C = 0.95)\n",
    "logit.fit(Zs_train, ys_train)\n",
    "print(\"The score for logistic regression is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*logit.score(Zs_train, ys_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*logit.score(Zs_test, ys_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes... with scaling the test r2 goes down to 32%!! Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C = 0.95)\n",
    "logit.fit(Zs_train_scaled, ys_train)\n",
    "print(\"The score for logistic regression is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*logit.score(Zs_train_scaled, ys_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*logit.score(Zs_test, ys_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "knn_confusion = confusion_matrix(knn.predict(Zs_test), ys_test)\n",
    "fig, ax = plt.subplots(dpi=90)\n",
    "#plt.figure(dpi=90)\n",
    "sns.heatmap(knn_confusion, cmap=plt.cm.Blues, annot=True, square=True)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('kNN confusion matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ys_predict = knn.predict(Zs_test)\n",
    "print(\"Default threshold:\")\n",
    "print(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(ys_test, ys_predict), \n",
    "                                                     recall_score(ys_test, ys_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<a class=\"anchor\" id=\"metrics\"></a>\n",
    "## Metrics\n",
    "\n",
    "Accuracy:% obs correctly classified/all observations\n",
    "\n",
    "Precision: How many selected items are relevant?\n",
    "\n",
    "Recall:How many relevant items are selected?\n",
    "\n",
    "F1 Score: Harmonic mean b/w precision & recall\n",
    "\n",
    "Confusion Matrix: Accuracy by class\n",
    "\n",
    "ROC Curve: Sensitivity vs specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img src=https://www.digital-mr.com/media/cache/5e/b4/5eb4dbc50024c306e5f707736fd79c1e.png width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img src=https://miro.medium.com/max/722/1*pk05QGzoWhCgRiiFbz-oKQ.png width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img src=https://newbiettn.github.io/images/confusion-matrix-noted.jpg width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**TPR** = true positive rate = sensitivity\n",
    "\n",
    "**FPR** = false positive rate = specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Confusion Matrix Above**\n",
    "\n",
    "True Negative  | False Positive\n",
    "\n",
    "False Negative | True Positive\n",
    "\n",
    " 0Acutal 0pred  | 0Actual 1pred\n",
    " \n",
    " 1Actual 0pred  | 1Actual 1pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**My model**\n",
    "\n",
    "Right now precision is better than recall. The model is also often predicting discharge, because there is class imbalance. \n",
    "\n",
    "Precision: Truly admitted / truly admitted + we predicted they would be admitted and they were not\n",
    "Recall: Truly admitted/ truly admitted + we predicted they would not be admitted but they were \n",
    "\n",
    "Recall is more important in my case. I want to capture all those admitted. If we guess a few to be admitted, but they aren't that isn't as pertinent. In that case we're just over-allocating some resources. Would be nice to have both, though.\n",
    "\n",
    "FBeta: Setting an fbeta above 1 allows for the model to prioritize recall, so I'm going to see if 2,3, or 4 work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<a class=\"anchor\" id=\"features\"></a>\n",
    "## Feature Engineering\n",
    "\n",
    "Feature Enginering and Selection in Linked Notebook:\n",
    "\n",
    "- [Feature Engineering](project_3_features.ipynb)\n",
    "- [Prelim Models](#prelim)\n",
    "\n",
    "Results of Feature Engineering:\n",
    "\n",
    "- Dummy variables for categoricals\n",
    "- Most vitals did not follow a normal distribution, so I binned them into low, normal, high (for ex.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<a class=\"anchor\" id=\"prelim\"></a>\n",
    "**Prelim Models After Feature Engineering**\n",
    "\n",
    "- No cross validation\n",
    "- Only logistic and knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(Xs_train, ys_train)\n",
    "print(\"The score for kNN is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*knn.score(X_train, y_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C = 0.95)\n",
    "logit.fit(X_train, y_train)\n",
    "print(\"The score for logistic regression is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*logit.score(X_train, y_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*logit.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logit_confusion = confusion_matrix(logit.predict(X_test), y_test)\n",
    "fig, ax = plt.subplots(dpi=90)\n",
    "#plt.figure(dpi=90)\n",
    "sns.heatmap(logit_confusion, cmap=plt.cm.Blues, annot=True, square=True)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Log Reg confusion matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_predict = logit.predict(X_test)\n",
    "print(\"Default threshold:\")\n",
    "print(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_test, y_predict), \n",
    "                                                     recall_score(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Definitely an improvement! Recall looking quite a bit better. Going to try a few more models tomorrow to check out the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<a class=\"anchor\" id=\"modfunctions\"></a>\n",
    "## Functions (Model Selection Pipeline)\n",
    "\n",
    "Things to address in pipeline:\n",
    "- Class Imbalance\n",
    "- Kfold cross validation (small dataset)\n",
    "- Metrics selected above (recall important)\n",
    "- ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nick/Downloads/Project_3/select_cols.p', 'rb') as read_file:\n",
    "    selected_columns = pickle.load(read_file)\n",
    "\n",
    "tw = selected_columns.sample(50000)\n",
    "ttw = selected_columns.sample(1000)\n",
    "tttw = selected_columns.sample(50)\n",
    "\n",
    "X, Xs, y, ys, Z, Zs = selected_columns.drop(columns=['>50K', 'YRMARR', 'PWTYPE', 'BPLD', 'DEGFIELDD', 'EDUCD', 'PWMET13', 'BIRTHYR', 'SLWT', 'BPLD', 'RELATED', 'ANCESTR1D', 'EMPSTATD', 'PWMET13ERR', 'COUNTYICP']), tw.drop(columns=['>50K', 'YRMARR', 'PWTYPE', 'BPLD', 'DEGFIELDD', 'EDUCD', 'PWMET13', 'BIRTHYR', 'SLWT', 'BPLD', 'RELATED', 'ANCESTR1D', 'EMPSTATD', 'PWMET13ERR', 'COUNTYICP']), selected_columns.filter(['>50K']), tw.filter(['>50K']), selected_columns.filter(['EDUC', 'UHRSWORK', 'OCC', 'VALUEH', 'DEGFIELD', 'AGE', 'SEX', 'RACE', 'TRANWORK']), tw.filter(['EDUC', 'UHRSWORK', 'OCC', 'VALUEH', 'DEGFIELD', 'AGE', 'SEX', 'RACE', 'TRANWORK'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, ys, test_size=0.2, random_state=42)\n",
    "Z_train, Z_test, q_train, q_test = train_test_split(Z, y, test_size=0.2, random_state=42)\n",
    "Zs_train, Zs_test, qs_train, qs_test = train_test_split(Zs, ys, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = Zs_train.copy()\n",
    "train_df['$$$'] = ys_train\n",
    "\n",
    "std = StandardScaler()\n",
    "X_train_scaled = std.fit_transform(X_train)\n",
    "Xs_train_scaled = std.fit_transform(Xs_train)\n",
    "Z_train_scaled = std.fit_transform(Z_train)\n",
    "Zs_train_scaled = std.fit_transform(Zs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def models(Zs,ys,resampler):\n",
    "    '''\n",
    "    This function takes features, predictors, and a resampling method. It splits into train and test then\n",
    "    cross validates on the training set using kfolds = 5. Train test split and kfolds are stratified to \n",
    "    ensure representation of classes in the test set. Output is the FBeta score for each model (beta = 4)\n",
    "    prioritizing for recall. Also outputs ROC curve including all models\n",
    "    ------\n",
    "    Inputs: array, series, over_sampler (ADASYN, SMOTE, or RandomOverSampling)\n",
    "    Outputs: dictionary, plot\n",
    "    \n",
    "    '''\n",
    "    # 80:20 split holding out test set. Calling kfold\n",
    "    Zs_train, Zs_test, ys_train, ys_test = train_test_split(Zs,ys, test_size = 0.2, random_state = 42, stratify = y)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle = True, random_state=42)\n",
    "\n",
    "    # Dictionary to hold Results\n",
    "    cv_results = defaultdict()\n",
    "    cv_accuracy = defaultdict()\n",
    "    test_results = defaultdict()\n",
    "\n",
    "    # Get indices for split\n",
    "    Zs_train = np.array(Zs_train)\n",
    "    ys_train = np.array(ys_train)\n",
    "\n",
    "    # Couple models\n",
    "    models = {'logreg':LogisticRegression(C = 1, penalty = 'l1', solver = 'liblinear'), # See Grid Search CV\n",
    "             'knn': KNeighborsClassifier(),\n",
    "             'svc': svm.SVC(gamma = 'scale', probability = True),\n",
    "             'naive': BernoulliNB(),\n",
    "             'dectree': DecisionTreeClassifier(),\n",
    "             'forrest': RandomForestClassifier(n_estimators = 100),} \n",
    "             #'xgboost': XGBClassifier()}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        cv_results[model_name] = []\n",
    "        cv_accuracy[model_name] = []\n",
    "        \n",
    "        for indices in kf.split(Zs_train, ys_train):\n",
    "            train_ind = indices[0]\n",
    "            val_ind = indices[1]\n",
    "            Zs_tr, ys_tr = Zs_train[train_ind], ys_train[train_ind]\n",
    "            Zs_resampled_train, ys_resampled_train = resampler(random_state=42).fit_sample(Zs_tr, ys_tr)\n",
    "            Zs_val, ys_val = Zs_train[val_ind], ys_train[val_ind]\n",
    "            Zs_resampled_val, ys_resampled_val = resampler(random_state=42).fit_sample(Zs_val, ys_val)\n",
    "            model.fit(Zs_resampled_train, ys_resampled_train)\n",
    "            ys_pred = model.predict(Zs_val)\n",
    "            cv_results[model_name].append(fbeta_score(ys_val, ys_pred,4))\n",
    "            cv_results[model_name].append('Precision Score {:.2f}'.format(precision_score(ys_val, ys_pred)))\n",
    "            cv_results[model_name].append('Recall Score: {:.2f}'.format(recall_score(ys_val, ys_pred)))\n",
    "            cv_results[model_name].append('F1 Score: {:.2f}'.format(f1_score(ys_val, ys_pred)))\n",
    "            cv_results[model_name].append('Training Accuracy:{:6.2f}%'.format(100*model.score(Zs_train, ys_train)))\n",
    "            cv_results[model_name].append('Validation Accuracy: {:6.2f}%'.format(100*model.score(Zs_val, ys_val)))\n",
    "\n",
    "            cv_accuracy[model_name].append(100*model.score(Zs_val, ys_val))\n",
    "    fig = plt.figure(figsize=(10,10))    \n",
    "    for model_name, model in models.items():\n",
    "        cv_results[model_name] = 'Mean FBeta Score: {:.2f}'.format(statistics.mean(cv_results[model_name]))\n",
    "        cv_accuracy[model_name] = 'Mean Val Accuracy: {:6.2f}%'.format(statistics.mean(cv_accuracy[model_name]))\n",
    "        test_results[model_name] = 'Test Accuracy: {:6.2f}%'.format(100*model.score(Zs_test, ys_test))\n",
    "        fpr, tpr, threshold_curve = roc_curve(ys_test, model.predict_proba(X_test)[::,1])\n",
    "        plt.plot(fpr, tpr,lw=2, color =np.random.rand(3,), label = model_name)\n",
    "        plt.legend()\n",
    "        plt.plot([0,1],[0,1],c='violet',ls='--')\n",
    "        plt.xlim([-0.05,1.05])\n",
    "        plt.ylim([-0.05,1.05])\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "        plt.title('ROC Curve for All Models');\n",
    "    return cv_results, cv_accuracy, test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<a class=\"anchor\" id=\"modelselection\"></a>\n",
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Random Over Sampler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "models(Zs_train, ys_train, RandomUnderSampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Undersampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "models(X,y,ADASYN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "models(X,y,SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "^^^ Change oversampling methods above to undersampling for below 50k! ^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Taking Out KTAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_dktas = X.drop([2,3,4,5], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "models(X_dktas,y, ADASYN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Local vs Regional Hospital**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_region = df_fe[df_fe['hospital'] == 1].reset_index()\n",
    "df_local = df_fe[df_fe['hospital'] == 0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_local = df_local.drop(['hospital','admissions', 'index'], axis = 1)\n",
    "y_local = df_local['admissions']\n",
    "X_region = df_region.drop(['hospital','admissions', 'index'], axis = 1)\n",
    "y_region = df_region['admissions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "models(X_local, y_local, RandomOverSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "models(X_region, y_region, RandomOverSampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Looks like the model is better at perfoming in the regional hospital... so I did some research:\n",
    "\n",
    "Study setting\n",
    "We selected 1 regional and 1 local ED based on a number of factors, including patient visits per year, the presence of emergency nurses dedicated to triage only, and collection possibility of the selected variables. Both EDs were academic urban medical centers. The regional ED had approximately 45,000 patient visits per year, and the local ED had approximately 40,000 patient visits per year. Both EDs were divided into a triage area at the entrance and a treatment area. In the regional ED, emergency nurses and doctors took a medical history and performed a physical examination together. The data collected were recorded in the initial nursing record and the doctor’s record, respectively. In the local ED, an emergency nurse conducted a physical examination alone. Initial nursing records from the two EDs included chief complaint, onset time, arrival mode, underlying disease, vital signs, oxygen saturation, surgical history, mental state, pain score, medication and allergy history. The pain scale used in the two EDs was the Numeric Rating Scale (NRS) which consists of a patient self-reporting 11-point scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let's see if I take out KTAS which is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_local_dktas = X_local.drop([2,3,4,5], axis = 1)\n",
    "X_region_dktas = X_region.drop([2,3,4,5], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Performance by Hospital without KTAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "models(X_local_dktas, y_local, SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "models(X_region_dktas, y_region, SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<a class=\"anchor\" id=\"gridsearch\"></a>\n",
    "## Grid Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "logreg = LogisticRegression()\n",
    "clf = GridSearchCV(logreg, hyperparameters, cv=5, verbose=0)\n",
    "best_model = clf.fit(Zs_train, ys_train)    \n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Think about vif individual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<a class=\"anchor\" id=\"logfunctions\"></a>\n",
    "## Logistic Regression Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def logregPlots(X,y, resampler):\n",
    "    # Fit logistic regression\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n",
    "    \n",
    "    # Stratify ensures minority class is represented in both sets in 80:20 ratio\n",
    "    logit = LogisticRegression(C = 1, penalty = 'l1', solver = 'liblinear')\n",
    "    \n",
    "    # Get indices for split\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    cv_results = []\n",
    "    for indices in kf.split(X_train, y_train):\n",
    "        train_ind = indices[0]\n",
    "        val_ind = indices[1]\n",
    "        X_tr, y_tr = X_train[train_ind], y_train[train_ind]\n",
    "        X_resampled_train, y_resampled_train = resampler(random_state=42).fit_sample(X_tr,y_tr)\n",
    "        X_val, y_val = X_train[val_ind], y_train[val_ind]\n",
    "        X_resampled_val, y_resampled_val = resampler(random_state=42).fit_sample(X_val, y_val)\n",
    "        logit.fit(X_resampled_train, y_resampled_train)\n",
    "        y_pred = logit.predict(X_val)\n",
    "        cv_results.append(#'Precision Score {:.2f}'.format(precision_score(y_val, y_pred)),\n",
    "                                  #'Recall Score: {:.2f}'.format(recall_score(y_val, y_pred)), \n",
    "                                  #'F1 Score: {:.2f}'.format(f1_score(y_val, y_pred)), \n",
    "                                #'Training Accuracy:{:6.2f}%'.format(100*model.score(X_train, y_train)),\n",
    "                                #'Validation Accuracy: {:6.2f}%'.format(100*model.score(X_val, y_val)),\n",
    "                                 fbeta_score(y_val, y_pred,4))\n",
    "    cv_results = 'Training FBeta Score: {:.2f}'.format(statistics.mean(cv_results))\n",
    "    \n",
    "    # Start Figure\n",
    "    fig = plt.figure(figsize = (15,15))\n",
    "    spec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.rc('axes', titlesize = 14, labelsize = 14)\n",
    "        \n",
    "    # Make predictions on y_test\n",
    "    logit_confusion = confusion_matrix(logit.predict(X_test), y_test)\n",
    "    ax1 = fig.add_subplot(spec[0,0])\n",
    "    sns.heatmap(logit_confusion, cmap=plt.cm.Blues, annot=True, square=True)\n",
    "    bottom, top = ax1.get_ylim()\n",
    "    ax1.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Log Reg Confusion Matrix');\n",
    "    \n",
    "    #ROC Curve\n",
    "    ax2 = fig.add_subplot(spec[0,1])\n",
    "    fpr, tpr, threshold_curve = roc_curve(y_test, logit.predict_proba(X_test)[:,1] )\n",
    "    plt.plot(fpr, tpr,lw=2, color = 'navy')\n",
    "    plt.plot([0,1],[0,1],c='violet',ls='--')\n",
    "    plt.xlim([-0.05,1.05])\n",
    "    plt.ylim([-0.05,1.05])\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC Curve for Logistic Regression');\n",
    "    \n",
    "    # Feature Importance\n",
    "    ax3 = fig.add_subplot(spec[1,0])\n",
    "    viz = FeatureImportances(logit, labels = ['Elderly', 'Middle Age', 'High BT', \n",
    "                                              'Gender', 'High RR', colormap = plt.cm.RdBu)\n",
    "    viz.fit(X, y)\n",
    "    plt.title('Feature Importance of 20 Features')\n",
    "    plt.xlabel('Relative Coefficient Magnitude')\n",
    "\n",
    "    # Precision/Recall Curve\n",
    "    ax4 = fig.add_subplot(spec[1,1])\n",
    "    pc, rc, threshold_curve = precision_recall_curve(y_test, logit.predict_proba(X_test)[:,1])\n",
    "    plt.plot(threshold_curve, pc[:-1], lw = 2, label = 'Precision', color = 'navy')\n",
    "    plt.plot(threshold_curve, rc[:-1], lw = 2, label = 'Recall', color = 'navy')\n",
    "    plt.plot([0,1],[0,1],c='violet',ls='--')\n",
    "    plt.xlim([-0.05,1.05])\n",
    "    plt.ylim([-0.05,1.05])\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Precision/Recall Curve for Logistic Regression');\n",
    "    \n",
    "    # Position Subplots\n",
    "    #plt.subplots_adjust(right = 2.2, top = 1, wspace = 0.5)\n",
    "    \n",
    "    #Metrics\n",
    "    plt.savefig(\"logreg.png\")\n",
    "    print(\"ROC AUC score = \", roc_auc_score(y_test, logit.predict_proba(X_test)[:,1]))\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def logregMetrics(X,y,resampler):\n",
    "    # 80:20 split holding out test set. Calling kfold\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle = True, random_state=42)\n",
    "\n",
    "    # Get indices for split\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    # Couple models\n",
    "    log_reg = LogisticRegression(C = 1, penalty = 'l1', solver = 'liblinear')\n",
    "    \n",
    "    for train_ind, val_ind in kf.split(X_train, y_train):\n",
    "        X_tr, y_tr = X_train[train_ind], y_train[train_ind]\n",
    "        X_resampled_train, y_resampled_train = resampler(random_state=42).fit_sample(X_tr,y_tr)\n",
    "        X_val, y_val = X_train[val_ind], y_train[val_ind]\n",
    "        X_resampled_val, y_resampled_val = resampler(random_state=42).fit_sample(X_val, y_val)\n",
    "        log_reg.fit(X_resampled_train, y_resampled_train)\n",
    "        y_pred = log_reg.predict(X_val)\n",
    "        training_result = ('Training Precision Score {:.2f}'.format(precision_score(y_val, y_pred)),\n",
    "                                  'Training Recall Score: {:.2f}'.format(recall_score(y_val, y_pred)), \n",
    "                                  #'F1 Score: {:.2f}'.format(f1_score(y_val, y_pred)), \n",
    "                                 'Training FBeta Score: {:.2f}'.format(fbeta_score(y_val, y_pred,2)))\n",
    "    y_pred_test = log_reg.predict(X_test)\n",
    "    test_result = ('Test Precision Score {:.2f}'.format(precision_score(y_test, y_pred_test)),\n",
    "                                  'Test Recall Score: {:.2f}'.format(recall_score(y_test, y_pred_test)), \n",
    "                                  #'F1 Score: {:.2f}'.format(f1_score(y_val, y_pred)), \n",
    "                                 'Test FBeta Score: {:.2f}'.format(fbeta_score(y_test, y_pred_test,2)))\n",
    "\n",
    "    return training_result, test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<a class=\"anchor\" id=\"logregperf\"></a>\n",
    "## Logistic Regression Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logreg(X,y, SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logregPlots(X,y, SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logregPlots(X_dktas, y, ADASYN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<a class=\"anchor\" id=\"pickle\"></a>\n",
    "## Pickling the Model for Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def logregPickle(X,y):\n",
    "    log_reg = LogisticRegression(C = 1, penalty = 'l1', solver = 'liblinear')\n",
    "    log_reg.fit(X,y)\n",
    "    log_reg.feature_names = X.columns\n",
    "    log_reg.target_names = ['Not Admitted', 'Admitted']\n",
    "    with open('project_3/website/model.pkl', 'wb') as to_write:\n",
    "        pickle.dump(log_reg, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logregPickle(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<a class=\"anchor\" id=\"predict\"></a>\n",
    "## Checking Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open('website/model.pkl', 'rb') as read_file:\n",
    "    lr_model = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "lr_model.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-03-21T18:21:34.886267Z",
     "start_time": "2019-03-21T18:21:34.873107Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_model.predict([[0,1,1,0,1,1,0,0,1,0,1,1,0,1,0,0,0,1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-03-21T18:21:35.272784Z",
     "start_time": "2019-03-21T18:21:35.262266Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_model.predict_proba([[0,1,1,0,1,1,0,0,1,0,1,1,0,1,0,0,0,1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "lr_model.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pred_probs = lr_model.predict_proba([[0,1,1,0,1,1,0,0,1,0,1,1,0,1,0,0,0,1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for index in np.argsort(pred_probs)[::-1]:\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "feature_names = lr_model.feature_names\n",
    "feature_dict = dict((feature,0) for feature in feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "feature_dict['sex'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x_input = [\n",
    "        float(feature_dict.get(name, 0)) for name in lr_model.feature_names\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "feature_dict = dict((feature,0) for feature in lr_model.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for name in lr_model.feature_names:\n",
    "    print(feature_dict[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
